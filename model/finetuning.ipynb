{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install torch transformers accelerate bitsandbytes datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install transformers accelerate datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install transformers accelerate torch datasets\n",
    "! pip install bitsandbytes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "# Load the quantized model\n",
    "model_path = \"mistral-13b-v0.1.Q3_K_M.gguf\"\n",
    "llm = Llama(model_path=model_path)\n",
    "\n",
    "# Example prompt\n",
    "prompt = \"Explain the role of a Pulmonologist.\"\n",
    "output = llm(prompt, max_tokens=100)\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load symptom data\n",
    "with open('scraped_data_subsections.json', 'r', encoding='utf-8') as file1:\n",
    "    symptom_data = json.load(file1)\n",
    "\n",
    "# Load doctor data\n",
    "with open('cleaned_doctors_data_multiple_locations_cleaned.json', 'r', encoding='utf-8') as file2:\n",
    "    doctor_data = json.load(file2)\n",
    "\n",
    "# Initialize fine-tuning data\n",
    "fine_tuning_data = []\n",
    "\n",
    "# Process symptom data\n",
    "for entry in symptom_data:\n",
    "    if 'subsections' in entry:\n",
    "        for subsection in entry['subsections']:\n",
    "            heading = subsection.get('heading', 'No Heading')\n",
    "            content = ' '.join(subsection.get('content', []))\n",
    "\n",
    "            # Add user prompts and completions\n",
    "            fine_tuning_data.extend([\n",
    "                {\"prompt\": f\"What should I do if I have {heading}?\", \"completion\": content},\n",
    "                {\"prompt\": f\"Can you explain {heading}?\", \"completion\": content},\n",
    "                {\"prompt\": f\"Is {heading} serious?\", \"completion\": content}\n",
    "            ])\n",
    "\n",
    "# Process doctor data\n",
    "for entry in doctor_data:\n",
    "    doctor_name = entry.get('Doctor Name', 'Unknown Doctor')\n",
    "    specialty = entry.get('Specialty', 'General')\n",
    "    location = entry.get('Location', 'Unknown Location')\n",
    "    address = entry.get('Address', 'Unknown Address')\n",
    "\n",
    "    # Add user prompts and completions\n",
    "    fine_tuning_data.extend([\n",
    "        {\"prompt\": f\"Give me a list of {specialty.lower()}s in {location}.\",\n",
    "         \"completion\": f\"Here is a {specialty.lower()} in {location}: {doctor_name} at {address}.\"},\n",
    "        {\"prompt\": f\"Who is a good {specialty.lower()} in {location}?\",\n",
    "         \"completion\": f\"{doctor_name} is a well-known {specialty.lower()} in {location}, located at {address}.\"},\n",
    "        {\"prompt\": f\"Where can I find a {specialty.lower()} near {location}?\",\n",
    "         \"completion\": f\"You can find a {specialty.lower()} in {location}: {doctor_name} at {address}.\"}\n",
    "    ])\n",
    "\n",
    "# Save as JSONL\n",
    "with open('fine_tuning_data.jsonl', 'w', encoding='utf-8') as output_file:\n",
    "    for entry in fine_tuning_data:\n",
    "        output_file.write(json.dumps(entry) + '\\n')\n",
    "\n",
    "print(\"Fine-tuning data has been generated successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "import os\n",
    "\n",
    "# Path to the local model and tokenizer\n",
    "local_model_path = \"mistral-13b-v0.1.Q3_K_M.gguf\"  # Replace with your exact model path\n",
    "\n",
    "# Load the tokenizer from the local path\n",
    "tokenizer = AutoTokenizer.from_pretrained(local_model_path, trust_remote_code=True)\n",
    "\n",
    "# Load the model from the local path\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    local_model_path,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",  # Use GPU if available\n",
    "    load_in_8bit=True   # Ensure 8-bit mode for quantized model\n",
    ")\n",
    "\n",
    "# Prepare the model for LoRA fine-tuning\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# Define LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=16,  # LoRA rank\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # Adjust for your model architecture\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "# Apply LoRA\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Load your fine-tuning dataset\n",
    "dataset = load_dataset(\"json\", data_files=\"fine_tuning_data.jsonl\", split=\"train\")\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"prompt\"], text_target=examples[\"completion\"], truncation=True, max_length=512)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    "    per_device_train_batch_size=2,  # Adjust as per hardware\n",
    "    num_train_epochs=3,\n",
    "    gradient_accumulation_steps=4,\n",
    "    fp16=True,\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    ")\n",
    "\n",
    "# Start fine-tuning\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned model\n",
    "model.save_pretrained(\"./fine_tuned_model\")\n",
    "tokenizer.save_pretrained(\"./fine_tuned_model\")\n",
    "\n",
    "print(\"Fine-tuning completed successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\navi\\Master-Thesis\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\navi\\.cache\\huggingface\\hub\\models--meta-llama--Llama-2-13b-hf. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Downloading shards: 100%|██████████| 3/3 [35:09<00:00, 703.33s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:22<00:00,  7.53s/it]\n",
      "Some parameters are on the meta device because they were offloaded to the disk and cpu.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"meta-llama/Llama-2-13b-hf\"  # Replace with your chosen model\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.10s/it]\n",
      "Some parameters are on the meta device because they were offloaded to the disk and cpu.\n",
      "You shouldn't move a model that is dispatched using accelerate hooks.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "You can't move a model that has some modules offloaded to cpu or disk.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 25\u001b[0m\n\u001b[0;32m     22\u001b[0m model\u001b[38;5;241m.\u001b[39mgradient_checkpointing_enable()\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Move model to the appropriate device (Ensure layers are fully loaded)\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_available\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Load fine-tuning dataset\u001b[39;00m\n\u001b[0;32m     28\u001b[0m dataset \u001b[38;5;241m=\u001b[39m load_dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjson\u001b[39m\u001b[38;5;124m\"\u001b[39m, data_files\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfine_tuning_data.jsonl\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n",
      "File \u001b[1;32mc:\\Users\\navi\\Master-Thesis\\venv\\Lib\\site-packages\\accelerate\\big_modeling.py:460\u001b[0m, in \u001b[0;36mdispatch_model.<locals>.add_warning.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    458\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m param \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mparameters():\n\u001b[0;32m    459\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m param\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 460\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt move a model that has some modules offloaded to cpu or disk.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    461\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: You can't move a model that has some modules offloaded to cpu or disk."
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# Disable symlink warnings\n",
    "os.environ[\"HF_HUB_DISABLE_SYMLINKS_WARNING\"] = \"1\"\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model_name = \"meta-llama/Llama-2-13b-hf\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",  # Automatically distribute layers\n",
    "    torch_dtype=torch.float16,  # Use mixed precision to save memory\n",
    "    low_cpu_mem_usage=True  # Load with reduced CPU memory\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "# Enable gradient checkpointing\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "# Move model to the appropriate device (Ensure layers are fully loaded)\n",
    "model = model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load fine-tuning dataset\n",
    "dataset = load_dataset(\"json\", data_files={\"train\": \"fine_tuning_data.jsonl\"})\n",
    "\n",
    "# LoRA configuration\n",
    "config = LoraConfig(\n",
    "    r=4,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],  # Target Mistral/LLaMA 2 layers\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "# Apply LoRA\n",
    "lora_model = get_peft_model(model, config)\n",
    "print(\"LoRA configuration applied successfully!\")\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./fine_tuned_model\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=8,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-5,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    fp16=True,  # Use mixed precision\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "# Ensure model is moved off the meta device\n",
    "lora_model = lora_model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=lora_model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned model\n",
    "lora_model.save_pretrained(\"./fine_tuned_model\")\n",
    "print(\"Fine-tuning completed and model saved!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
